{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bndGlSz3pCvV",
        "bZn4coQOpzYO",
        "Kp4IDVxPqKRH",
        "fm7hE8t_qn8C",
        "aiTsKBodrxRu",
        "tiI65-WssUac",
        "T_U53sTusxdu",
        "799fvCVf3_4J",
        "3TNMn0ykLsBA",
        "WVxzsfhZQMrP",
        "7Zcr8viRQj90",
        "F6TDm3FWQy2f",
        "RdKqoN9DRTvl",
        "B611Q1JWOI0L",
        "wWN-n_6BPBAH"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1Ô∏è‚É£: Generate a conversation\n",
        "\n",
        "Generate a conversation between customer and the support agent (any domain) using deepseek - R1 Model"
      ],
      "metadata": {
        "id": "FIlhkSdILB83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1Ô∏è‚É£: Install Required Libraries\n"
      ],
      "metadata": {
        "id": "bndGlSz3pCvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsv7t-EJoqko",
        "outputId": "1bd3bf2d-3acf-4a94-cc9d-c6a10004ecc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Checking and installing Ollama...\n",
            "‚úÖ Ollama installed successfully!\n",
            "üîÑ Verifying Ollama installation...\n",
            "‚úÖ Ollama is installed and ready!\n",
            "üîÑ Starting Ollama server...\n",
            "‚úÖ Ollama server started with PID: 2809\n",
            "üîÑ Downloading DeepSeek-R1 model...\n",
            "‚ùå Failed to download DeepSeek-R1 model. Retrying...\n",
            "‚úÖ DeepSeek-R1 model successfully downloaded!\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Install Ollama if not installed\n",
        "print(\"üîÑ Checking and installing Ollama...\")\n",
        "try:\n",
        "    subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True, check=True)\n",
        "    print(\"‚úÖ Ollama installed successfully!\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ö†Ô∏è Error installing Ollama: {str(e)}\")\n",
        "\n",
        "# Verify Ollama installation\n",
        "print(\"üîÑ Verifying Ollama installation...\")\n",
        "try:\n",
        "    subprocess.run(\"ollama --version\", shell=True, check=True)\n",
        "    print(\"‚úÖ Ollama is installed and ready!\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"‚ùå Ollama is not installed properly. Please install it manually from https://ollama.com/download\")\n",
        "\n",
        "# Start Ollama in the background\n",
        "print(\"üîÑ Starting Ollama server...\")\n",
        "try:\n",
        "    ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    print(f\"‚úÖ Ollama server started with PID: {ollama_process.pid}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error starting Ollama: {str(e)}\")\n",
        "\n",
        "# Ensure the DeepSeek-R1 model is available\n",
        "print(\"üîÑ Downloading DeepSeek-R1 model...\")\n",
        "try:\n",
        "    subprocess.run(\"ollama pull deepseek-r1\", shell=True, check=True)\n",
        "    print(\"‚úÖ DeepSeek-R1 model is ready!\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"‚ùå Failed to download DeepSeek-R1 model. Retrying...\")\n",
        "\n",
        "    # Retry model download using alternative command\n",
        "    try:\n",
        "        subprocess.run(\"ollama pull deepseek-r1\", shell=True, check=True)\n",
        "        print(\"‚úÖ DeepSeek-R1 model successfully downloaded!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå DeepSeek-R1 download failed again: {str(e)}\")\n",
        "        print(\"üîπ Please ensure your internet connection is stable.\")\n",
        "        print(\"üîπ Run `!ollama list` to check available models.\")\n",
        "\n",
        "\n",
        "\n",
        "# Why This is Needed\n",
        "# Installs ollama, starts the AI model server, and downloads the deepseek-r1 model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2Ô∏è‚É£: Import Required Libraries"
      ],
      "metadata": {
        "id": "bZn4coQOpzYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama\n",
        "\n",
        "import ollama  # Ollama for DeepSeek-R1 model interaction\n",
        "import json  # For storing conversation history\n",
        "import time  # For adding natural response delays\n",
        "import subprocess  # For running system commands\n",
        "\n",
        "# Why This is Needed\n",
        "# ollama ‚Üí Handles chat generation.\n",
        "# json ‚Üí Stores conversation history in Step 4 and Step 7.\n",
        "# subprocess ‚Üí Runs Ollama commands for AI responses."
      ],
      "metadata": {
        "id": "5_8rLlFGp3_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a09f199-060a-48cb-d74d-937d4845cf3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.10.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3Ô∏è‚É£: Initialize Chat History Storage\n"
      ],
      "metadata": {
        "id": "Kp4IDVxPqKRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []  # Stores chat history for context retention\n"
      ],
      "metadata": {
        "id": "RZ1UKTDD0hbL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " This ensures the bot can generate a full back-and-forth conversation without predefined text.\n"
      ],
      "metadata": {
        "id": "2ApQNf7ppvFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4Ô∏è‚É£: Define the Chatbot Response Function"
      ],
      "metadata": {
        "id": "fm7hE8t_qn8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_input):\n",
        "    \"\"\"\n",
        "    Generates chatbot responses using DeepSeek-R1 via Ollama.\n",
        "    Stores conversation history for summarization in Step 8.\n",
        "    \"\"\"\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})  # Store user input\n",
        "\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=\"deepseek-r1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a professional customer support agent.\"},\n",
        "                *conversation_history  # Pass conversation history for context\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        bot_reply = response['message']['content']  # Extract chatbot's response\n",
        "\n",
        "        # Store chatbot response\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
        "\n",
        "        return bot_reply  # Return response to display\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Error: {str(e)}\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2ZNN7j4qy86"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Why is This Needed?\n",
        "‚úÖ Stores chat history for better AI responses.\n",
        "‚úÖ Ensures chatbot acts professionally in its responses."
      ],
      "metadata": {
        "id": "3tghpjlZ1er9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5Ô∏è‚É£: Define Auto-Generated Conversation\n",
        "MODIFIED: Now stores conversation history in JSON format."
      ],
      "metadata": {
        "id": "aiTsKBodrxRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_auto_conversation():\n",
        "    \"\"\"\n",
        "    Generates a back-and-forth customer support conversation using DeepSeek-R1.\n",
        "    \"\"\"\n",
        "    user_prompt = (\n",
        "        \"Simulate a realistic customer support conversation where a customer reports an issue with a purchased product. \"\n",
        "        \"The support agent should assist, provide solutions, and conclude the chat naturally within 7-10 exchanges.\"\n",
        "    )\n",
        "\n",
        "    # Store the auto-generated conversation prompt as user input\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=\"deepseek-r1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a professional customer support agent.\"},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        bot_reply = response['message']['content']  # Extract chatbot's response\n",
        "\n",
        "        # Store the full auto-generated conversation\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
        "\n",
        "        print(f\"\\nüìå **Auto-Generated Conversation:**\\n\\n{bot_reply}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"‚ö†Ô∏è Error generating auto-conversation: {str(e)}\\n\"\n",
        "            \"‚ùó **Make sure Ollama is installed and running.**\\n\"\n",
        "            \"üìå Visit: https://ollama.com/download for installation help.\"\n",
        "        )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "13g2SgMSsBZF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Why is This Needed?\n",
        "\n",
        "‚úÖ Allows \"auto\" mode to generate realistic customer support conversations.\n",
        "\n"
      ],
      "metadata": {
        "id": "o48ZWaYTsLdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6Ô∏è‚É£: Define the Chat Function"
      ],
      "metadata": {
        "id": "tiI65-WssUac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    \"\"\"\n",
        "    Handles manual and auto-generated chat interactions.\n",
        "    Ensures the user chooses between 'auto' and manual chat mode.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ü§ñ **Welcome to the AI-Powered Customer Support Chatbot!**\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nüîπ **How to use:**\")\n",
        "    print(\"   - Type your query and press Enter to chat manually.\")\n",
        "    print(\"   - Type **'auto'** to generate a full automatic conversation.\")\n",
        "    print(\"   - Type **'exit'** anytime to end the chat.\")\n",
        "\n",
        "    # üîπ Ensure the chatbot **WAITs for user input before execution**\n",
        "    while True:\n",
        "        mode = input(\"\\nüîπ Would you like to chat manually or use 'auto' mode? (Type 'auto' or press Enter for manual chat): \").strip().lower()\n",
        "\n",
        "        if mode in [\"\", \"auto\"]:\n",
        "            break  # Valid input, exit loop\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Invalid input! Please type 'auto' for an automatic chat or press Enter for manual chat.\")\n",
        "\n",
        "    # üîπ Now process the input correctly\n",
        "    if mode == \"auto\":\n",
        "        print(\"\\nüîÑ **Generating a sample conversation...**\\n\")\n",
        "        generate_auto_conversation()  # Generates realistic customer-agent chat\n",
        "\n",
        "    else:  # Manual chat mode\n",
        "        print(\"\\nüîπ **Starting Manual Chat Mode**. Type your queries below. (Type 'exit' to end chat)\\n\")\n",
        "        while True:\n",
        "            user_input = input(\"\\nüë§ Customer: \")\n",
        "\n",
        "            if user_input.lower() == \"exit\":\n",
        "                print(\"\\nü§ñ Support Bot: Goodbye! Have a great day! üëã\")\n",
        "                return\n",
        "\n",
        "            bot_response = generate_response(user_input)\n",
        "            print(f\"\\nü§ñ Support Bot: {bot_response}\")\n"
      ],
      "metadata": {
        "id": "SR3ijDnlsZkL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7Ô∏è‚É£: Run the Chatbot"
      ],
      "metadata": {
        "id": "T_U53sTusxdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama\n",
        "if __name__ == \"__main__\":\n",
        "    chat()  # Explicitly call chat() only when the script is run interactively\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4segQakEqTRC",
        "outputId": "234d7d8a-538a-4989-d614-c9524b3e8cab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.10.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
            "\n",
            "==================================================\n",
            "ü§ñ **Welcome to the AI-Powered Customer Support Chatbot!**\n",
            "==================================================\n",
            "\n",
            "üîπ **How to use:**\n",
            "   - Type your query and press Enter to chat manually.\n",
            "   - Type **'auto'** to generate a full automatic conversation.\n",
            "   - Type **'exit'** anytime to end the chat.\n",
            "\n",
            "üîπ Would you like to chat manually or use 'auto' mode? (Type 'auto' or press Enter for manual chat): auto\n",
            "\n",
            "üîÑ **Generating a sample conversation...**\n",
            "\n",
            "\n",
            "üìå **Auto-Generated Conversation:**\n",
            "\n",
            "<think>\n",
            "Okay, so I just got this message from a user asking me to simulate a realistic customer support conversation where a customer reports an issue with a purchased product. They want the interaction to be natural, with 7-10 exchanges.\n",
            "\n",
            "First, I need to figure out how to approach this. The user is probably testing my ability to handle customer service scenarios effectively. Maybe they're looking to see if I can provide good solutions or understand common issues customers face.\n",
            "\n",
            "I should start by thinking about the typical flow of such a conversation. The customer would likely reach out with a problem, express frustration, and then work through it step by step until resolved.\n",
            "\n",
            "I'll need to make up a plausible issue someone might have. Let's say it's an expired software license or activation error after purchasing online. That seems common enough and allows for clear steps to resolve the issue.\n",
            "\n",
            "The first message should be from the customer complaining about their product not working. I'll greet them and acknowledge their problem.\n",
            "\n",
            "Next, I need to respond empathetically, apologize if necessary, and explain what's happening with their purchase.\n",
            "\n",
            "Then, guide them through checking their license details again or contact support for further assistance.\n",
            "\n",
            "I should keep each exchange concise but clear, ensuring the conversation flows naturally without being too long. Since they asked for 7-10 exchanges, I'll aim for about that range to cover all necessary steps from initial complaint to resolution.\n",
            "\n",
            "Also, at the end, it's good practice to thank them and offer further help if needed, which adds a nice touch of professionalism.\n",
            "\n",
            "I have to make sure the language is friendly and helpful throughout. Maybe add some emojis like a computer or checkmark to make it more engaging.\n",
            "\n",
            "Let me structure this:\n",
            "\n",
            "1. Customer reports an issue.\n",
            "2. Support agent acknowledges and empathizes.\n",
            "3. Asks for details on the product.\n",
            "4. Customer provides license info.\n",
            "5. Agent checks system, finds expired info.\n",
            "6. Offers next steps like reactivating or contacting support.\n",
            "7. Encourages them to reach out again.\n",
            "\n",
            "That should cover it in about 7-8 messages, which is within their request range.\n",
            "\n",
            "I need to avoid making the agent sound too robotic or unhelpful. It's all about sounding natural and supportive while providing clear solutions.\n",
            "\n",
            "Also, I should vary my responses a bit to keep it engaging but still professional.\n",
            "\n",
            "Alright, time to put this together in a conversational format.\n",
            "</think>\n",
            "\n",
            "Certainly! Here's a simulated customer support conversation based on the scenario:\n",
            "\n",
            "---\n",
            "\n",
            "**Customer:** Hi, I'm having trouble with my purchased software. It's not working after I tried to activate it yesterday.\n",
            "\n",
            "**Support Agent:** Hello! I'm sorry to hear that you're experiencing issues with your product. Let me assist you in resolving this matter.\n",
            "\n",
            "**Customer:** My license key expired on October 15th.\n",
            "\n",
            "**Support Agent:** Ah, I see. There seems to be an issue with the activation or reactivation of your license. Could you please provide me with your product details and the activation code?\n",
            "\n",
            "**Customer:** Sure, it's called \"TechGuard Pro\" version 2.3, and my activation code is ABC123456.\n",
            "\n",
            "**Support Agent:** Thank you for providing that information. Upon reviewing the details, I noticed that your license key expired on October 15th as well. \n",
            "\n",
            "**Customer:** Oh no, it does! That's why it wasn't working when I tried to activate it yesterday.\n",
            "\n",
            "**Support Agent:** I understand how frustrating that must be. There are a couple of options available for you:\n",
            "\n",
            "1. **Reactiving the License:** You can reactivate your license using the product key if it is still valid.\n",
            "2. **Contacting Support:** If reactivation isn't feasible, our support team will help resolve this issue promptly.\n",
            "\n",
            "**Customer:** Okay, I'll try to reactivate first. If that doesn't work, I'll contact you.\n",
            "\n",
            "**Support Agent:** That sounds like a solid plan. Please proceed with the reactivation process and let me know if you need further assistance.\n",
            "\n",
            "**Customer:** Thanks for your help!\n",
            "\n",
            "**Support Agent:** You're welcome! Feel free to reach out again if anything else comes up. Have a great day!\n",
            "\n",
            "---\n",
            "\n",
            "This interaction is natural, empathetic, and provides clear solutions while maintaining a friendly tone.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8Ô∏è‚É£: Save and Print the conversation"
      ],
      "metadata": {
        "id": "799fvCVf3_4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Save and Print Conversation\n",
        "\n",
        "def save_conversation_to_file(conversation_data, file_path='stored_conversation.txt'):\n",
        "    \"\"\"\n",
        "    Save the conversation history to a file in a readable format and print the conversation.\n",
        "\n",
        "    Args:\n",
        "        conversation_data (list): The conversation history to be saved.\n",
        "        file_path (str): The file path where the conversation will be stored.\n",
        "    \"\"\"\n",
        "    global content  # Declare content as a global variable so it can be accessed later\n",
        "    try:\n",
        "        content = \"\"  # Initialize content as an empty string\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            for message in conversation_data:\n",
        "                role = \"User\" if message[\"role\"] == \"user\" else \"Assistant\"\n",
        "                message_content = message[\"content\"]\n",
        "\n",
        "                # Append the conversation to the content variable\n",
        "                content += f\"{role}: {message_content}\\n\\n\"\n",
        "\n",
        "                # Write the conversation to the file\n",
        "                file.write(f\"{role}: {message_content}\\n\\n\")\n",
        "\n",
        "        print(f\"‚úÖ Conversation successfully saved to '{file_path}'.\")\n",
        "\n",
        "        # --- Printing Part ---\n",
        "        print(\"\\nüìå The conversation history is as follows:\\n\")\n",
        "        for message in conversation_data:\n",
        "            role = \"User\" if message[\"role\"] == \"user\" else \"Assistant\"\n",
        "            print(f\"{role}: {message['content']}\\n\")  # This is the printing part\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error saving conversation: {str(e)}\")\n",
        "\n",
        "\n",
        "# Triggering the conversation saving process after Step 7\n",
        "save_conversation_to_file(conversation_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niD5UjT9TztF",
        "outputId": "4ab76110-2997-4ff5-fd11-7ae6b18788a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversation successfully saved to 'stored_conversation.txt'.\n",
            "\n",
            "üìå The conversation history is as follows:\n",
            "\n",
            "User: Simulate a realistic customer support conversation where a customer reports an issue with a purchased product. The support agent should assist, provide solutions, and conclude the chat naturally within 7-10 exchanges.\n",
            "\n",
            "Assistant: <think>\n",
            "Okay, so I just got this message from a user asking me to simulate a realistic customer support conversation where a customer reports an issue with a purchased product. They want the interaction to be natural, with 7-10 exchanges.\n",
            "\n",
            "First, I need to figure out how to approach this. The user is probably testing my ability to handle customer service scenarios effectively. Maybe they're looking to see if I can provide good solutions or understand common issues customers face.\n",
            "\n",
            "I should start by thinking about the typical flow of such a conversation. The customer would likely reach out with a problem, express frustration, and then work through it step by step until resolved.\n",
            "\n",
            "I'll need to make up a plausible issue someone might have. Let's say it's an expired software license or activation error after purchasing online. That seems common enough and allows for clear steps to resolve the issue.\n",
            "\n",
            "The first message should be from the customer complaining about their product not working. I'll greet them and acknowledge their problem.\n",
            "\n",
            "Next, I need to respond empathetically, apologize if necessary, and explain what's happening with their purchase.\n",
            "\n",
            "Then, guide them through checking their license details again or contact support for further assistance.\n",
            "\n",
            "I should keep each exchange concise but clear, ensuring the conversation flows naturally without being too long. Since they asked for 7-10 exchanges, I'll aim for about that range to cover all necessary steps from initial complaint to resolution.\n",
            "\n",
            "Also, at the end, it's good practice to thank them and offer further help if needed, which adds a nice touch of professionalism.\n",
            "\n",
            "I have to make sure the language is friendly and helpful throughout. Maybe add some emojis like a computer or checkmark to make it more engaging.\n",
            "\n",
            "Let me structure this:\n",
            "\n",
            "1. Customer reports an issue.\n",
            "2. Support agent acknowledges and empathizes.\n",
            "3. Asks for details on the product.\n",
            "4. Customer provides license info.\n",
            "5. Agent checks system, finds expired info.\n",
            "6. Offers next steps like reactivating or contacting support.\n",
            "7. Encourages them to reach out again.\n",
            "\n",
            "That should cover it in about 7-8 messages, which is within their request range.\n",
            "\n",
            "I need to avoid making the agent sound too robotic or unhelpful. It's all about sounding natural and supportive while providing clear solutions.\n",
            "\n",
            "Also, I should vary my responses a bit to keep it engaging but still professional.\n",
            "\n",
            "Alright, time to put this together in a conversational format.\n",
            "</think>\n",
            "\n",
            "Certainly! Here's a simulated customer support conversation based on the scenario:\n",
            "\n",
            "---\n",
            "\n",
            "**Customer:** Hi, I'm having trouble with my purchased software. It's not working after I tried to activate it yesterday.\n",
            "\n",
            "**Support Agent:** Hello! I'm sorry to hear that you're experiencing issues with your product. Let me assist you in resolving this matter.\n",
            "\n",
            "**Customer:** My license key expired on October 15th.\n",
            "\n",
            "**Support Agent:** Ah, I see. There seems to be an issue with the activation or reactivation of your license. Could you please provide me with your product details and the activation code?\n",
            "\n",
            "**Customer:** Sure, it's called \"TechGuard Pro\" version 2.3, and my activation code is ABC123456.\n",
            "\n",
            "**Support Agent:** Thank you for providing that information. Upon reviewing the details, I noticed that your license key expired on October 15th as well. \n",
            "\n",
            "**Customer:** Oh no, it does! That's why it wasn't working when I tried to activate it yesterday.\n",
            "\n",
            "**Support Agent:** I understand how frustrating that must be. There are a couple of options available for you:\n",
            "\n",
            "1. **Reactiving the License:** You can reactivate your license using the product key if it is still valid.\n",
            "2. **Contacting Support:** If reactivation isn't feasible, our support team will help resolve this issue promptly.\n",
            "\n",
            "**Customer:** Okay, I'll try to reactivate first. If that doesn't work, I'll contact you.\n",
            "\n",
            "**Support Agent:** That sounds like a solid plan. Please proceed with the reactivation process and let me know if you need further assistance.\n",
            "\n",
            "**Customer:** Thanks for your help!\n",
            "\n",
            "**Support Agent:** You're welcome! Feel free to reach out again if anything else comes up. Have a great day!\n",
            "\n",
            "---\n",
            "\n",
            "This interaction is natural, empathetic, and provides clear solutions while maintaining a friendly tone.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2Ô∏è‚É£ Generation of the Summary\n",
        "Generate a conversation summary and extract meaningful insights"
      ],
      "metadata": {
        "id": "OzGKT5JhJwtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1Ô∏è‚É£: Setting Up Google Colab Environment\n",
        "Set up the environment."
      ],
      "metadata": {
        "id": "3TNMn0ykLsBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the environment.\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('pip install transformers > /dev/null 2>&1')\n",
        "os.system('pip install sentencepiece > /dev/null 2>&1')\n",
        "os.system('pip install accelerate > /dev/null 2>&1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9AXNF-8LyeV",
        "outputId": "4c5412b7-f10f-4805-b4bb-9f6ef6bb8393"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2Ô∏è‚É£: Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "WVxzsfhZQMrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import re\n"
      ],
      "metadata": {
        "id": "oGljmCLYQTuX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3Ô∏è‚É£: Loading the Model and Tokenizer\n",
        "The `facebook/bart-large-cnn` model and its tokenizer are loaded for processing the conversation text."
      ],
      "metadata": {
        "id": "7Zcr8viRQj90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The `facebook/bart-large-cnn` model and its tokenizer are loaded for processing the conversation text.\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    model_name = 'facebook/bart-large-cnn'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n"
      ],
      "metadata": {
        "id": "6l2Cskh7QpKu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4Ô∏è‚É£: Extracting Conversation from Content\n",
        "This function identifies where the conversation starts and extracts only the conversation portion."
      ],
      "metadata": {
        "id": "F6TDm3FWQy2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function identifies where the conversation starts and extracts only the conversation portion.\n",
        "\n",
        "def extract_conversation(text):\n",
        "    conversation_start = re.search(r\"Customer:.*\", text)\n",
        "\n",
        "    if conversation_start:\n",
        "        conversation = text[conversation_start.start():].strip()\n",
        "        return conversation\n",
        "    else:\n",
        "        raise ValueError(\"No valid conversation found in the content.\")\n"
      ],
      "metadata": {
        "id": "yJtE0tT3WFyz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5Ô∏è‚É£: Cleaning and Splitting Text\n",
        "\n",
        "This function cleans the text, removes unwanted characters, and breaks it into sentences for better coherence."
      ],
      "metadata": {
        "id": "RdKqoN9DRTvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function cleans the text, removes unwanted characters, and breaks it into sentences for better coherence.\n",
        "\n",
        "def clean_and_split_text(text):\n",
        "    text = text.replace('\\n', ' ').replace('**', '').strip()\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "cCterbwSWbZw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6Ô∏è‚É£: Summarization Function\n",
        "This function processes the conversation, breaks it into manageable chunks,\n",
        "and summarizes each chunk before combining the results.\n"
      ],
      "metadata": {
        "id": "B611Q1JWOI0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function processes the conversation, breaks it into manageable chunks,\n",
        "# and summarizes each chunk before combining the results.\n",
        "\n",
        "\n",
        "def generate_summary(text):\n",
        "    if len(text.strip()) == 0:\n",
        "        raise ValueError(\"The input text is empty. Please provide valid content.\")\n",
        "\n",
        "    conversation = extract_conversation(text)\n",
        "    sentences = clean_and_split_text(conversation)\n",
        "    conversation_text = ' '.join(sentences)\n",
        "\n",
        "    max_chunk_size = 800\n",
        "    chunks = [conversation_text[i:i + max_chunk_size] for i in range(0, len(conversation_text), max_chunk_size)]\n",
        "    final_summary = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=300,\n",
        "            min_length=50,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        final_summary.append(summary)\n",
        "\n",
        "    combined_summary = \" \".join(final_summary)\n",
        "    formatted_summary = \"\\n\".join(combined_summary[i:i+100] for i in range(0, len(combined_summary), 100))\n",
        "    return formatted_summary\n"
      ],
      "metadata": {
        "id": "SbxI2h0SXgq9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7Ô∏è‚É£: Generating the Summary\n",
        "This is the final step where we call the summarization function and display the output.\n"
      ],
      "metadata": {
        "id": "wWN-n_6BPBAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the final step where we call the summarization function and display the output.\n",
        "\n",
        "try:\n",
        "    summary = generate_summary(content)\n",
        "    print(\"Generated Summary:\\n\")\n",
        "    print(summary)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_5DlJblPNX-",
        "outputId": "12866b19-71d2-43e4-f0ec-f98f6cbe075f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "\n",
            "Customer: My license key expired on October 15th. Support Agent: Ah, I see. There seems to be an iss\n",
            "ue with the activation or reactivation of your license. Could you please provide me with your produc\n",
            "t details and the activation code? Customer: Sure, it's called \"TechGuard Pro\" version 2.3. to activ\n",
            "ate it yesterday. Support Agent: I understand how frustrating that must be. There are a couple of op\n",
            "tions available for you. You can reactivate your license using the product key if it is still valid.\n",
            " If reactivation isn't feasible, our support team will help resolve this issue promptly. .ng a frien\n",
            "dly tone. A friendly tone is a way of saying 'thank you' or 'I love you' for your kind words. It's a\n",
            "lso a way to say 'I'm sorry' for something you've done or said.\n"
          ]
        }
      ]
    }
  ]
}